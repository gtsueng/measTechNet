{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90689a95",
   "metadata": {},
   "source": [
    "# Measurement Techniques Graph - Preparing the data\n",
    "\n",
    "Measurement Techniques are currently scattered across multiple different ontologies. In order to improve the mapping of measTech terms to ontology terms, the measurement techniques across multiple ontologies need to be combined to reduce the likelihood of treating synonymous terms as multiple separate entities\n",
    "\n",
    "To do this, we will:\n",
    "\n",
    "1. Convert measurement technique ontology branches into subject (parent) predicate (has subclass) object (child) triples.\n",
    "2. Use NCBO BioPortals to map synonymous terms between ontologies to de-duplicate nodes\n",
    "3. Use term similarity and shared nodes to identify potential synonymous terms for de-duplication\n",
    "4. iterate on 3 until the graph is relatively unique\n",
    "\n",
    "This notebook covers the network generation using the triples and mapping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ac16c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46d6621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = os.getcwd()\n",
    "raw_path = os.path.join(script_path,'raw_files')\n",
    "raw_file_list = os.listdir(raw_path)\n",
    "result_path = os.path.join(script_path,'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d295500",
   "metadata": {},
   "outputs": [],
   "source": [
    "high2low_priority = [\"MMO\", \"CHMO\",\"OBI\",\"BAO\",\"EFO\",\"topic\",\"NCIT\"]\n",
    "low2high_priority = [\"NCIT\",\"topic\",\"EFO\",\"BAO\",\"OBI\",\"CHMO\",\"MMO\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5093ea",
   "metadata": {},
   "source": [
    "## Load the triples and de-dup between ontologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a70a4a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         subject_id  \\\n",
      "10  http://www.bioassayontology.org/bao#BAO_0002540   \n",
      "74  http://www.bioassayontology.org/bao#BAO_0010204   \n",
      "\n",
      "                                       predicate_id  \\\n",
      "10  http://www.w3.org/2000/01/rdf-schema#subClassOf   \n",
      "74  http://www.w3.org/2000/01/rdf-schema#subClassOf   \n",
      "\n",
      "                                          object_id  \\\n",
      "10  http://www.bioassayontology.org/bao#BAO_0000248   \n",
      "74  http://www.bioassayontology.org/bao#BAO_0003008   \n",
      "\n",
      "                               subject             object   predicate  \n",
      "10  ECL western blotting detection kit          assay kit  subClassOf  \n",
      "74         transporter substrate assay  transporter assay  subClassOf  \n"
     ]
    }
   ],
   "source": [
    "all_measTech_triples = pd.read_csv(os.path.join(result_path,'measTechOnly_mapped_triples.tsv'),delimiter='\\t',header=0,index_col=0)\n",
    "print(all_measTech_triples.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc1d6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640\n"
     ]
    }
   ],
   "source": [
    "sameAsSubset = all_measTech_triples.loc[all_measTech_triples['predicate']=='sameAs']\n",
    "#sameAsSubset.to_csv(os.path.join(result_path,'measTechOnlyMappings.tsv'),sep='\\t',header=True)\n",
    "print(len(sameAsSubset))\n",
    "#print(sameAsSubset.head(n=2))\n",
    "#print(sameAsSubset.groupby(['subject','object','predicate']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3e916da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ordered_mapping(high2low_priority,low2high_priority,sameAsSubset):\n",
    "    ordered_mapping = pd.DataFrame(columns=['subject_id','predicate_id','object_id','subject','predicate','object'])\n",
    "    i = 0\n",
    "    while i < len(low2high_priority):\n",
    "        inclusion_list = low2high_priority[i:len(low2high_priority)]\n",
    "        eachonto = low2high_priority[i]\n",
    "        tempSubset = sameAsSubset.loc[sameAsSubset['subject_id'].astype(str).str.contains(eachonto)]\n",
    "        reordered = pd.DataFrame(columns=['subject_id','predicate_id','object_id','subject','predicate','object'])\n",
    "        for eachpriority in high2low_priority[0:len(high2low_priority)-i]:\n",
    "            tempdf = tempSubset.loc[tempSubset['object_id'].astype(str).str.contains(eachpriority)]\n",
    "            tmpdf = tempdf.loc[~tempdf['object_id'].astype(str).str.contains(\"EMMO\")]\n",
    "            reordered = pd.concat((reordered,tmpdf),ignore_index=True)\n",
    "            unique_map = reordered.drop_duplicates(subset='subject_id',keep='first')\n",
    "        for includeonto in inclusion_list:\n",
    "            include_map = unique_map.loc[unique_map['object_id'].astype(str).str.contains(includeonto)]\n",
    "            ordered_mapping = pd.concat((ordered_mapping,include_map),ignore_index=True)\n",
    "        i=i+1\n",
    "    return ordered_mapping\n",
    "\n",
    "def export_ordered_mapping_dict(result_path,ordered_mapping):\n",
    "    ordered_mapping_dict = dict(zip(ordered_mapping.subject_id, ordered_mapping.object_id))\n",
    "    with open(os.path.join(result_path,'ordered_mapping_dict.json'),'w') as outwrite:\n",
    "        outwrite.write(json.dumps(ordered_mapping_dict,indent=4))\n",
    "    return ordered_mapping_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fdf5147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n"
     ]
    }
   ],
   "source": [
    "ordered_mapping = generate_ordered_mapping(high2low_priority,low2high_priority,sameAsSubset)\n",
    "print(len(ordered_mapping))\n",
    "ordered_mapping_dict = export_ordered_mapping_dict(result_path,ordered_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4673df07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8678\n"
     ]
    }
   ],
   "source": [
    "## clean up the triples\n",
    "lineage_triples = all_measTech_triples.loc[all_measTech_triples['predicate']=='subClassOf'].copy()\n",
    "print(len(lineage_triples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be919699",
   "metadata": {},
   "source": [
    "### Iteratively reduce duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7cac23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://purl.obolibrary.org/obo/OBI_0002119\n",
      "['http://purl.obolibrary.org/obo/CHMO_0000591']\n",
      "http://purl.obolibrary.org/obo/MMO_0000709 http://purl.obolibrary.org/obo/MMO_0000709\n",
      "203 29\n",
      "231\n"
     ]
    }
   ],
   "source": [
    "### Load the manual mappings\n",
    "with open(os.path.join(result_path,'results_from_cytoscape','mappings_found_via_network.json'),'r') as infile:\n",
    "    manual_map = json.load(infile)\n",
    "print(list(manual_map.keys())[0])\n",
    "\n",
    "### check for overlap in ordered_mapping_dict\n",
    "overlap = list(set(list(ordered_mapping_dict.keys())).intersection(set(list(manual_map.keys()))))\n",
    "print(overlap)\n",
    "print(ordered_mapping_dict[overlap[0]], manual_map[overlap[0]])\n",
    "#### The overlapping mapped values are the same, it's fine to merge\n",
    "\n",
    "### Merge the dictionaries\n",
    "print(len(list(ordered_mapping_dict.keys())), len(list(manual_map.keys())))\n",
    "ordered_mapping_dict.update(manual_map)\n",
    "print(len(list(ordered_mapping_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9e30206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8685 7\n",
      "8692\n",
      "8685\n"
     ]
    }
   ],
   "source": [
    "### Load the manual triples\n",
    "manual_triples = pd.read_csv(os.path.join(result_path,'results_from_cytoscape','triples_found_via_network.tsv'),header=0,delimiter='\\t')\n",
    "#print(manual_triples.head(n=2))\n",
    "#print(lineage_triples.head(n=2))\n",
    "### Merge the manual triples\n",
    "print(len(lineage_triples), len(manual_triples))\n",
    "lineage_triples = pd.concat((lineage_triples,manual_triples),ignore_index=True)\n",
    "print(len(lineage_triples))\n",
    "lineage_triples.drop_duplicates(keep='first',inplace=True)\n",
    "print(len(lineage_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc5fda2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 8659\n",
      "2 8659\n",
      "3 8659\n",
      "4 8659\n",
      "5 8659\n",
      "6 8659\n",
      "7 8659\n"
     ]
    }
   ],
   "source": [
    "## iteratively replace values in the triples and de-duplicate\n",
    "\n",
    "i=0\n",
    "lineage_triples['subject_id'] = lineage_triples['subject_id'].replace(to_replace=ordered_mapping_dict)\n",
    "lineage_triples['object_id'] = lineage_triples['object_id'].replace(to_replace=ordered_mapping_dict)\n",
    "clean_triples = lineage_triples.drop_duplicates(keep='first').copy()\n",
    "while i < len(low2high_priority):\n",
    "    clean_triples['subject_id'] = clean_triples['subject_id'].replace(to_replace=ordered_mapping_dict)\n",
    "    clean_triples['object_id'] = clean_triples['object_id'].replace(to_replace=ordered_mapping_dict)\n",
    "    clean_triples = clean_triples.drop_duplicates(keep='first').copy()\n",
    "    i=i+1\n",
    "    print(i, len(clean_triples))\n",
    "    #print(i, len(clean_triples.loc[clean_triples['subject_id'].astype(str).str.contains('EFO')]))\n",
    "\n",
    "#example_dict = {'http://purl.obolibrary.org/obo/NCIT_C114102': 'http://www.ebi.ac.uk/efo/EFO_0009638', 'http://purl.obolibrary.org/obo/NCIT_C124040': 'http://www.ebi.ac.uk/efo/EFO_0010719'}\n",
    "#print(lineage_triples.loc[lineage_triples['subject_id']=='http://www.ebi.ac.uk/efo/EFO_0009638'])\n",
    "#print(clean_triples.loc[clean_triples['subject_id']=='http://www.ebi.ac.uk/efo/EFO_0009638'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78a26e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                id        name\n",
      "0            https://www.w3.org/2002/07/owl#sameAs      sameAs\n",
      "1  http://www.w3.org/2000/01/rdf-schema#subClassOf  subClassOf\n"
     ]
    }
   ],
   "source": [
    "name_map = pd.read_csv(os.path.join(result_path,'name_iri_map.tsv'),delimiter='\\t',header=0,index_col=0)\n",
    "print(name_map.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b29872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean up the labels for the triples\n",
    "tmpdf = clean_triples[['subject_id','predicate_id','object_id']].copy()\n",
    "def map_triples(clean_triples,name_map):\n",
    "    clean_triples.rename(columns={'subject':'subject_id','predicate':'predicate_id','object':'object_id'},inplace=True)\n",
    "    subject_map = name_map.copy()\n",
    "    subject_map.rename(columns={'name':'subject','id':'subject_id'},inplace=True)\n",
    "    predicate_map = name_map.copy()\n",
    "    predicate_map.rename(columns={'name':'predicate','id':'predicate_id'},inplace=True)\n",
    "    object_map = name_map.copy()\n",
    "    object_map.rename(columns={'name':'object','id':'object_id'},inplace=True)\n",
    "    tmpdf = clean_triples.merge(subject_map,on='subject_id',how='left')\n",
    "    tmp2df = tmpdf.merge(object_map,on='object_id',how='left')\n",
    "    tmp3df = tmp2df.merge(predicate_map,on='predicate_id',how='left')\n",
    "    mapped_triples = tmp3df.drop_duplicates(keep='first')\n",
    "    return mapped_triples\n",
    "\n",
    "clean_triples = map_triples(tmpdf,name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0fd3e40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        subject_id  \\\n",
      "0  http://www.bioassayontology.org/bao#BAO_0002540   \n",
      "1  http://www.bioassayontology.org/bao#BAO_0010204   \n",
      "\n",
      "                                      predicate_id  \\\n",
      "0  http://www.w3.org/2000/01/rdf-schema#subClassOf   \n",
      "1  http://www.w3.org/2000/01/rdf-schema#subClassOf   \n",
      "\n",
      "                                         object_id  \\\n",
      "0       http://purl.obolibrary.org/obo/OBI_0003369   \n",
      "1  http://www.bioassayontology.org/bao#BAO_0003008   \n",
      "\n",
      "                              subject             object   predicate  \n",
      "0  ECL western blotting detection kit          assay kit  subClassOf  \n",
      "1         transporter substrate assay  transporter assay  subClassOf  \n"
     ]
    }
   ],
   "source": [
    "print(clean_triples.head(n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6b695e",
   "metadata": {},
   "source": [
    "### Verify the cleaning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69a1938f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://purl.obolibrary.org/obo/NCIT_C101294', 'http://purl.obolibrary.org/obo/NCIT_C16681', 'http://edamontology.org/topic_4028', 'http://edamontology.org/topic_3177', 'http://edamontology.org/topic_3676', 'http://edamontology.org/topic_3169', 'http://edamontology.org/topic_3170', 'http://edamontology.org/topic_0177', 'http://edamontology.org/topic_2271', 'http://edamontology.org/topic_0183', 'http://edamontology.org/topic_3474', 'http://edamontology.org/topic_0221', 'http://edamontology.org/topic_4014', 'http://edamontology.org/topic_3077', 'http://edamontology.org/topic_0182', 'http://edamontology.org/topic_3385', 'http://edamontology.org/topic_4017', 'http://edamontology.org/topic_0133', 'http://edamontology.org/topic_3448', 'http://edamontology.org/topic_2828', 'http://edamontology.org/topic_0611', 'http://edamontology.org/topic_0134', 'http://edamontology.org/topic_4016', 'http://edamontology.org/topic_3452', 'http://www.ebi.ac.uk/efo/EFO_0010935', 'http://www.ebi.ac.uk/efo/EFO_0030037', 'http://www.ebi.ac.uk/efo/EFO_0004176', 'http://www.ebi.ac.uk/efo/EFO_0000395', 'http://www.ebi.ac.uk/efo/EFO_0000726', 'http://www.ebi.ac.uk/efo/EFO_0000663']\n",
      "http://edamontology.org/topic_3673\n",
      "0\n",
      "Empty DataFrame\n",
      "Columns: [subject_id, predicate_id, object_id, subject, object, predicate]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "test_list = list(ordered_mapping_dict.keys())\n",
    "test_df = lineage_triples.loc[(lineage_triples['subject_id'].isin(test_list[0:10])|(lineage_triples['object_id']).isin(test_list[0:10]))].copy()\n",
    "#print(test_list[0:30])\n",
    "print(ordered_mapping_dict[test_list[0]])\n",
    "print(len(test_df))\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e0c16d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [subject_id, predicate_id, object_id, subject, object, predicate]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "test_df['subject_id'] = test_df['subject_id'].replace(to_replace=ordered_mapping_dict)\n",
    "test_df['object_id'] = test_df['object_id'].replace(to_replace=ordered_mapping_dict)\n",
    "clean_triples = test_df.drop_duplicates(keep='first')\n",
    "print(clean_triples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6517133f",
   "metadata": {},
   "source": [
    "## Focus on nodes with multiple parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1cb868ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1172\n"
     ]
    }
   ],
   "source": [
    "## Get the nodes with multiple parents\n",
    "tmp = clean_triples.groupby(['subject_id']).size().reset_index(name=\"counts\")\n",
    "multi_parent = tmp.loc[tmp['counts']>1]\n",
    "print(len(multi_parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14a110fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4065\n"
     ]
    }
   ],
   "source": [
    "## get the immediate parents and children\n",
    "multilist = multi_parent['subject_id'].unique().tolist()\n",
    "p1f1 = clean_triples.loc[(clean_triples['subject_id'].isin(multilist))|(clean_triples['object_id'].isin(multilist))].copy()\n",
    "print(len(p1f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61853dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3043\n",
      "6061\n"
     ]
    }
   ],
   "source": [
    "## expand to the grandparents and grandchildren\n",
    "p1f1list = list(set(p1f1['object_id'].unique().tolist()).union(set(p1f1['subject_id'].unique().tolist())))\n",
    "print(len(p1f1list))\n",
    "p2f2 = clean_triples.loc[(clean_triples['subject_id'].isin(p1f1list))|(clean_triples['object_id'].isin(p1f1list))].copy()\n",
    "print(len(p2f2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2245f0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1172\n",
      "2558\n",
      "                                         subject_id  \\\n",
      "23      http://purl.obolibrary.org/obo/CHMO_0000063   \n",
      "30  http://www.bioassayontology.org/bao#BAO_0003003   \n",
      "\n",
      "                                       predicate_id  \\\n",
      "23  http://www.w3.org/2000/01/rdf-schema#subClassOf   \n",
      "30  http://www.w3.org/2000/01/rdf-schema#subClassOf   \n",
      "\n",
      "                                          object_id  \\\n",
      "23  http://www.bioassayontology.org/bao#BAO_0000050   \n",
      "30  http://www.bioassayontology.org/bao#BAO_0000042   \n",
      "\n",
      "                                      subject                     object  \\\n",
      "23  bioluminescence resonance energy transfer            bioluminescence   \n",
      "30                   cytokine secretion assay  Cue Signal Response assay   \n",
      "\n",
      "     predicate  \n",
      "23  subClassOf  \n",
      "30  subClassOf  \n",
      "2511\n"
     ]
    }
   ],
   "source": [
    "#### search for different nodes with same parent and same children\n",
    "\n",
    "## if subject is subClassOf 2 objects AND the 2 objects(now subjects) are subClassOf same object, keep\n",
    "\n",
    "## 1. Get the subjects that have 2 parents\n",
    "print(len(multi_parent))\n",
    "\n",
    "## 2. Get the parents of those subjects\n",
    "p1 = clean_triples.loc[clean_triples['subject_id'].isin(multilist)].copy()\n",
    "p1_list = clean_triples['object_id'].unique().tolist()\n",
    "#print(p1.head(n=2))\n",
    "\n",
    "## 3. Get the grandparents of those subjects\n",
    "p2 = clean_triples.loc[clean_triples['subject_id'].isin(p1_list)].copy()\n",
    "#print(p2)\n",
    "\n",
    "## 4. Get the subjects for which grandparents have multiple children\n",
    "tmp_kids = p1.groupby(['object_id']).size().reset_index(name='counts')\n",
    "multi_kids = tmp_kids.loc[tmp_kids['counts']>1]\n",
    "kidslist = tmp_kids['object_id'].unique().tolist()\n",
    "qualkids = p1.loc[p1['object_id'].isin(kidslist)]\n",
    "print(len(qualkids))\n",
    "print(qualkids.head(n=2))\n",
    "cleankids = qualkids.drop_duplicates(subset=['subject_id','object_id'],keep='first')\n",
    "print(len(cleankids))\n",
    "potential_syn = cleankids.copy()\n",
    "\n",
    "tmp_kids2 = p2.groupby(['object_id']).size().reset_index(name='counts')\n",
    "multi_kids2 = tmp_kids2.loc[tmp_kids2['counts']>1]\n",
    "kidslist2 = tmp_kids2['object_id'].unique().tolist()\n",
    "qualkids2 = p2.loc[p2['object_id'].isin(kidslist2)]\n",
    "cleankids2 = qualkids2.drop_duplicates(subset=['subject_id','object_id'],keep='first')\n",
    "potential_syn2 = cleankids2.copy()\n",
    "\n",
    "## 5. Assemble original subject (with multi-parents), with grandparents that have multiple kids\n",
    "#potential_syn = pd.concat((multi_parent,qualkids),ignore_index=True)\n",
    "#print(potential_syn.head(n=4))\n",
    "#print(len(potential_syn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf20385",
   "metadata": {},
   "source": [
    "## Export for Cytoscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21374b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs2export = {\"allnodes\":clean_triples,\"p1f1\":p1f1,\"p2f2\":p2f2,\"potential_synonyms\":potential_syn,\"potential_synonyms2\":potential_syn2}\n",
    "iteration_round = \"1\"\n",
    "def export_df(dfname,dfs2export):\n",
    "    df = dfs2export[dfname]\n",
    "    df.to_csv(os.path.join(result_path,'for_cytoscape',f'{dfname}_table_gen_{iteration_round}.tsv'),sep='\\t',header=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e0311e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional tables for cytoscape (unnecessary)\n",
    "\n",
    "def export_nodes(dfname,dfs2export):\n",
    "    df = dfs2export[dfname]\n",
    "    subject_table = df[['subject','subject_id']].copy()\n",
    "    subject_table.rename(columns={'subject':'name','subject_id':'id'},inplace=True)\n",
    "    object_table = df[['object','object_id']].copy()\n",
    "    object_table.rename(columns={'object':'name','object_id':'id'},inplace=True)\n",
    "    node_table = pd.concat((subject_table,object_table))\n",
    "    node_table.drop_duplicates(keep='first',inplace=True)\n",
    "    node_table.to_csv(os.path.join(result_path,'for_cytoscape',f'{dfname}_node_table.tsv'),sep='\\t',header=True)\n",
    "\n",
    "def export_edges(dfname,dfs2export):\n",
    "    df = dfs2export[dfname]\n",
    "    df['value'] = 1\n",
    "    edge_table = df[['predicate','predicate_id','value']].copy()\n",
    "    edge_table.rename(columns={'predicate':'name','predicate_id':'id'})\n",
    "    edge_table.drop_duplicates(keep='first',inplace=True)\n",
    "    edge_table.to_csv(os.path.join(result_path,'for_cytoscape',f'{dfname}_edge_table.tsv'),sep='\\t',header=True)\n",
    "    network_name_table = df[['subject','predicate','object']].copy()\n",
    "    network_name_table.to_csv(os.path.join(result_path,'for_cytoscape',f'{dfname}_network_name_table.tsv'),sep='\\t',header=True)\n",
    "    network_id_table = df[['subject_id','predicate_id','object_id']].copy()\n",
    "    network_id_table.to_csv(os.path.join(result_path,'for_cytoscape',f'{dfname}_network_id_table.tsv'),sep='\\t',header=True)\n",
    "    network_val_table = df[['subject','value','object']].copy()\n",
    "    network_val_table.to_csv(os.path.join(result_path,'for_cytoscape',f'{dfname}_network_val_table.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d849e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dfname in list(dfs2export.keys()):\n",
    "    #export_nodes(dfname,dfs2export)\n",
    "    #export_edges(dfname,dfs2export)\n",
    "    export_df(dfname,dfs2export)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cbfc66",
   "metadata": {},
   "source": [
    "## Mappings found based on cytoscape analysis:\n",
    "\n",
    "\n",
    "### From p1 network\n",
    "Microscopy Assay: http://purl.obolibrary.org/obo/OBI_0002119\n",
    "Microscopy: http://purl.obolibrary.org/obo/CHMO_0000067\n",
    "\n",
    "Tandem Mass spec: http://purl.obolibrary.org/obo/OBI_0003540\n",
    "LC MS: http://purl.obolibrary.org/obo/OBI_0003097 \n",
    "LC Tandem MS: http://purl.obolibrary.org/obo/CHMO_0000701 (subtype of tandem Mass/LCMS spec)\n",
    "\n",
    "### From networks going farther up\n",
    "Already in mappings:\n",
    "Array: http://www.ebi.ac.uk/efo/EFO_0002698\n",
    "Array assay: http://purl.obolibrary.org/obo/OBI_0001865\n",
    "\n",
    "DNA Array: http://www.ebi.ac.uk/efo/EFO_0002701\n",
    "DNA Microarray: http://purl.obolibrary.org/obo/OBI_0400148\n",
    "\n",
    "\n",
    "Relationships not in mappings:\n",
    "DNA Array http://www.ebi.ac.uk/efo/EFO_0002701 subClassOf nucleic acid array http://www.bioassayontology.org/bao#BAO_0000504\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d1f13a",
   "metadata": {},
   "source": [
    "### Questions raised from inspecting the knowledge graph\n",
    "* How to treat terms that are semantically different, but related? For example, the microarray device vs the microarray technique both exist in some ontologies and have different roots\n",
    "  * Decision: Since the purpose is for standardizing measurementTechniques, device terms should map to the technique terms whenever possible to minimize duplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3421fea7",
   "metadata": {},
   "source": [
    "## Analyze network with Networkx\n",
    "\n",
    "https://stackoverflow.com/questions/73154911/how-to-draw-a-graph-with-networkx-from-pandas-dataframe-with-node-size-depending\n",
    "\n",
    "How BioPortal mappings are done: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4159173/ (LOOM = Lexical mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b927534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "p1f1['values'] = 1\n",
    "g = nx.from_pandas_edgelist(p1f1, source=\"subject\", target=\"object\")\n",
    "\n",
    "d = p1f1.groupby(\"subject\")[\"values\"].sum().to_dict()\n",
    "for node in g.nodes:\n",
    "    d.setdefault(node, 1)\n",
    "\n",
    "nodes, values = zip(*d.items())\n",
    "nx.draw(g, nodelist=list(nodes), node_size=[v * 100 for v in values], with_labels=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706d8aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

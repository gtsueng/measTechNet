{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90689a95",
   "metadata": {},
   "source": [
    "# Measurement Techniques Graph - Preparing the data\n",
    "\n",
    "Measurement Techniques are currently scattered across multiple different ontologies. In order to improve the mapping of measTech terms to ontology terms, the measurement techniques across multiple ontologies need to be combined to reduce the likelihood of treating synonymous terms as multiple separate entities\n",
    "\n",
    "To do this, we will:\n",
    "\n",
    "1. Convert measurement technique ontology branches into subject (parent) predicate (has subclass) object (child) triples.\n",
    "2. Use NCBO BioPortals to map synonymous terms between ontologies to de-duplicate nodes\n",
    "3. Use term similarity and shared nodes to identify potential synonymous terms for de-duplication\n",
    "4. iterate on 3 until the graph is relatively unique\n",
    "\n",
    "This notebook covers the data processing for the conversion to triples and the mapping. Note that it depends on the mappings generated during the initial analysis of measurementTechniques (see measTechAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90ac16c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46d6621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = os.getcwd()\n",
    "parent_path = os.path.abspath(os.path.join(script_path, os.pardir))\n",
    "raw_path = os.path.join(script_path,'raw_files')\n",
    "map_path = os.path.join(parent_path,'measTechAnalysis','result','mappings')\n",
    "raw_file_list = os.listdir(raw_path)\n",
    "map_file_list = [x for x in os.listdir(map_path) if '.tsv' in x]\n",
    "result_path = os.path.join(script_path,'results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5093ea",
   "metadata": {},
   "source": [
    "## Create ordered mapping file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e08f608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NCIT is too big to fetch the mappings for, so generate it from the other mappings\n",
    "def generate_NCIT_mappings(map_path,map_file_list):\n",
    "    ncit = pd.DataFrame(columns=[\"source_id\",\"map_method\",\"target_id\"])\n",
    "    for eachmap in map_file_list:\n",
    "        df = pd.read_csv(os.path.join(map_path,eachmap), delimiter='\\t',header=0,index_col=0)\n",
    "        ncit_df = df.loc[df['target_id'].astype(str).str.contains('NCIT')]\n",
    "        ncit = pd.concat((ncit,ncit_df),ignore_index=True)\n",
    "    ncit.rename(columns = {\"target_id\":\"subject\",\"source_id\":\"object\"}, inplace=True)\n",
    "    ncit.rename(columns = {\"subject\":\"source_id\", \"object\":\"target_id\"}, inplace=True)\n",
    "    ncit.to_csv(os.path.join(map_path,'NCIT_mappings.tsv'),sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e73bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_NCIT_mappings(map_path,map_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f142af1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Filter mappings to just the relevant ones\n",
    "def create_mapping_df(map_path, mapping_ontos):\n",
    "    mapping_df = pd.DataFrame(columns=[\"source_ontology\",\"source_id\",\"map_method\",\"target_id\"])\n",
    "    for eachmap in mapping_ontos:\n",
    "        tmpdf = pd.read_csv(os.path.join(map_path,f\"{eachmap}_mappings.tsv\"),delimiter=\"\\t\",index_col=0,header=0)\n",
    "        ### get rid of same URIs\n",
    "        unique_map = tmpdf.loc[tmpdf['map_method'].astype(str).str.strip()!=\"SAME_URI\"]\n",
    "        ### limit maps to ones within ontology range\n",
    "        no_other_ontos = unique_map.loc[unique_map['target_id'].astype(str).str.contains(\"edamontology\") | \n",
    "                                         unique_map['target_id'].astype(str).str.contains(\"EFO\") |\n",
    "                                         unique_map['target_id'].astype(str).str.contains(\"CHMO\") |\n",
    "                                         unique_map['target_id'].astype(str).str.contains(\"MMO\") |\n",
    "                                         unique_map['target_id'].astype(str).str.contains(\"OBI\") |\n",
    "                                         unique_map['target_id'].astype(str).str.contains(\"BAO\") |\n",
    "                                         unique_map['target_id'].astype(str).str.contains(\"NCIT\")\n",
    "                                        ]\n",
    "        mapping_df = pd.concat((mapping_df,no_other_ontos),ignore_index=True)\n",
    "    no_dups = mapping_df.drop_duplicates(keep='first')\n",
    "    return no_dups                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48a484fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_ontos = [\"NCIT\",\"EDAM\",\"EFO\",\"BAO\",\"OBI\",\"CHMO\",\"MMO\"]\n",
    "mapping_df = create_mapping_df(map_path, mapping_ontos)\n",
    "clean_mapping_df = mapping_df.drop(columns='source_ontology')\n",
    "clean_mapping_df.to_csv(os.path.join(map_path,'all_mappings.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "effe3c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_mapping_df = pd.read_csv(os.path.join(map_path,'all_mappings.tsv'),delimiter='\\t',header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4534967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17120\n"
     ]
    }
   ],
   "source": [
    "print(len(clean_mapping_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665462fd",
   "metadata": {},
   "source": [
    "## Create iri/label dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5322ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dictionaries(raw_path,raw_file_list):\n",
    "    vocab = pd.DataFrame([{'id':'https://www.w3.org/2002/07/owl#sameAs','name':'sameAs'},\n",
    "                          {'id':'http://www.w3.org/2000/01/rdf-schema#subClassOf','name':'subClassOf'}])\n",
    "    for eachfile in raw_file_list:\n",
    "        df = pd.read_csv(os.path.join(raw_path,eachfile),header=0, usecols=['Class ID','Preferred Label'])\n",
    "        if eachfile == 'NCIT.csv':\n",
    "            df['Class ID'] = df['Class ID'].apply(lambda x: x.replace('http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#','http://purl.obolibrary.org/obo/NCIT_'))\n",
    "        df.rename(columns={'Preferred Label':'name','Class ID':'id'},inplace=True)\n",
    "        vocab = pd.concat((vocab,df),ignore_index=True)\n",
    "    clean_vocab = vocab.drop_duplicates(keep='first')\n",
    "    return clean_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29304b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                id        name\n",
      "0            https://www.w3.org/2002/07/owl#sameAs      sameAs\n",
      "1  http://www.w3.org/2000/01/rdf-schema#subClassOf  subClassOf\n",
      "255404\n"
     ]
    }
   ],
   "source": [
    "name_map = update_dictionaries(raw_path,raw_file_list)\n",
    "name_map.to_csv(os.path.join(result_path,'name_iri_map.tsv'),sep='\\t',header=True)\n",
    "print(name_map.head(n=2))\n",
    "print(len(name_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e64ec95",
   "metadata": {},
   "source": [
    "## Create triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc21c2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Class ID  \\\n",
      "0   http://purl.obolibrary.org/obo/CHEBI_50444   \n",
      "1  http://purl.obolibrary.org/obo/CHEBI_131787   \n",
      "\n",
      "                                      Parents  \n",
      "0  http://purl.obolibrary.org/obo/CHEBI_50218  \n",
      "1  http://purl.obolibrary.org/obo/CHEBI_48706  \n",
      "7773\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(map_path,map_file_list[0]),delimiter='\\t',header=0,index_col=0)\n",
    "df = pd.read_csv(os.path.join(raw_path,raw_file_list[0]),header=0, usecols=['Class ID','Parents'])\n",
    "\n",
    "print(df.head(n=2))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf3ed4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Address subclass of many triples delineated by \"|\"\n",
    "## These will only be in the object_id field and is a reason why there are missing object labels\n",
    "def split_multi_parents(objectvalue):\n",
    "    if \"|\" in objectvalue:\n",
    "        parents = objectvalue.split(\"|\")\n",
    "    else:\n",
    "        parents = [objectvalue]\n",
    "    return parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1a7dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_triples(df,reverse_mapping=False):\n",
    "    if 'map_method' in list(df.keys()):\n",
    "        ## This is a mapping file\n",
    "        no_self_dups = df.loc[df['map_method'].astype(str).str.strip()!=\"SAME_URI\"]\n",
    "        triple_df = no_self_dups[['source_id','target_id']].copy()\n",
    "        triple_df.rename(columns={'source_id':'subject','target_id':'object'},inplace=True)\n",
    "        triple_df['predicate']='https://www.w3.org/2002/07/owl#sameAs'\n",
    "        ## Generate a reverse mapping\n",
    "        if reverse_mapping==True:\n",
    "            tmpdf = no_self_dups[['source_id','target_id']].copy()\n",
    "            tmpdf.rename(columns={'source_id':'object','target_id':'subject'},inplace=True)\n",
    "            tmpdf['predicate']='https://www.w3.org/2002/07/owl#sameAs'\n",
    "            ## assemble together\n",
    "            triple_df = pd.concat((triple_df,tmpdf),ignore_index=True)\n",
    "        triple_df.drop_duplicates(keep='first',inplace=True)\n",
    "    if 'Parents' in list(df.keys()):\n",
    "        temp = df.loc[~df['Parents'].isna()].copy()\n",
    "        temp['object'] = temp.apply(lambda row: split_multi_parents(row['Parents']),axis=1)\n",
    "        triple_df = temp.explode('object')\n",
    "        triple_df.drop(columns='Parents',inplace=True)\n",
    "        triple_df.rename(columns={'Class ID':'subject'},inplace=True)\n",
    "        triple_df['predicate'] = 'http://www.w3.org/2000/01/rdf-schema#subClassOf'\n",
    "    return triple_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cb6021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_triples(raw_path, map_path, raw_file_list, map_file_list):\n",
    "    triple_df = pd.DataFrame(columns=['subject','predicate','object'])\n",
    "    for eachfile in raw_file_list:\n",
    "        df = pd.read_csv(os.path.join(raw_path,eachfile),header=0, usecols=['Class ID','Parents'])\n",
    "        if eachfile == 'NCIT.csv':\n",
    "            df['Class ID'] = df['Class ID'].apply(lambda x: x.replace('http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#','http://purl.obolibrary.org/obo/NCIT_'))\n",
    "            df['Parents'] = df['Parents'].apply(lambda x: x.replace('http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#','http://purl.obolibrary.org/obo/NCIT_'))\n",
    "        clean_df = convert_to_triples(df,False)\n",
    "        triple_df = pd.concat((triple_df,clean_df),ignore_index=True)\n",
    "    map_files = [x for x in map_file_list if x!='all_mappings.tsv']\n",
    "    for eachmap in map_files:\n",
    "        df = pd.read_csv(os.path.join(map_path,eachmap),delimiter='\\t',header=0,index_col=0)\n",
    "        clean_df = convert_to_triples(df)\n",
    "        triple_df = pd.concat((triple_df,clean_df),ignore_index=True)\n",
    "    clean_triples = triple_df.drop_duplicates(keep='first',inplace=False)\n",
    "    return clean_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f4dda79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325371\n"
     ]
    }
   ],
   "source": [
    "clean_triples = generate_triples(raw_path, map_path, raw_file_list, map_file_list)\n",
    "clean_triples.to_csv(os.path.join(result_path,'all_iri_triples.tsv'),sep='\\t',header=True)\n",
    "print(len(clean_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d274fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_triples(clean_triples,name_map):\n",
    "    clean_triples.rename(columns={'subject':'subject_id','predicate':'predicate_id','object':'object_id'},inplace=True)\n",
    "    subject_map = name_map.copy()\n",
    "    subject_map.rename(columns={'name':'subject','id':'subject_id'},inplace=True)\n",
    "    predicate_map = name_map.copy()\n",
    "    predicate_map.rename(columns={'name':'predicate','id':'predicate_id'},inplace=True)\n",
    "    object_map = name_map.copy()\n",
    "    object_map.rename(columns={'name':'object','id':'object_id'},inplace=True)\n",
    "    tmpdf = clean_triples.merge(subject_map,on='subject_id',how='left')\n",
    "    tmp2df = tmpdf.merge(object_map,on='object_id',how='left')\n",
    "    tmp3df = tmp2df.merge(predicate_map,on='predicate_id',how='left')\n",
    "    mapped_triples = tmp3df.drop_duplicates(keep='first')\n",
    "    return mapped_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b26798c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325988\n"
     ]
    }
   ],
   "source": [
    "mapped_triples = map_triples(clean_triples,name_map)\n",
    "print(len(mapped_triples))\n",
    "mapped_triples.to_csv(os.path.join(result_path,'all_ontologies_mapped.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4af94f7",
   "metadata": {},
   "source": [
    "### filtering the triples to measTech only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ae59069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "http://purl.obolibrary.org/obo/NCIT_C20368\n"
     ]
    }
   ],
   "source": [
    "### Get the base parent inclusion list\n",
    "parent_inclusion_list = []\n",
    "with open(os.path.join(result_path,'parent_inclusion_list.txt'),'r') as infile:\n",
    "    for line in infile:\n",
    "        parent_inclusion_list.append(line.strip())\n",
    "\n",
    "print(len(parent_inclusion_list))\n",
    "print(parent_inclusion_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d43d069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Iterate until all the descendants of the parent_inclusion_list are reached\n",
    "def generate_lineage_list(parent_inclusion_list, mapped_triples):\n",
    "    i = 0\n",
    "    all_parent_list = [x for x in parent_inclusion_list]\n",
    "    old_all_parent_list = len(set(all_parent_list))\n",
    "    tmpdf = mapped_triples.loc[mapped_triples['object_id'].isin(parent_inclusion_list)]\n",
    "    new_parent = tmpdf['subject_id'].unique().tolist()\n",
    "    all_parent_list.extend(new_parent)\n",
    "    while len(set(all_parent_list)) != old_all_parent_list:\n",
    "        tmpdf = mapped_triples.loc[mapped_triples['object_id'].isin(new_parent)]\n",
    "        new_parent = tmpdf['subject_id'].unique().tolist()\n",
    "        old_all_parent_list = len(set(all_parent_list))\n",
    "        all_parent_list.extend(new_parent)\n",
    "        i = i+1\n",
    "\n",
    "    all_unique_parents = list(set(all_parent_list))\n",
    "    print(\"iterations run: \",i,\" iterative # of records included: \",len(all_parent_list),\" unique records included: \",len(all_unique_parents))\n",
    "    return all_unique_parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1254c381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations run:  13  iterative # of records included:  270458  unique records included:  58238\n"
     ]
    }
   ],
   "source": [
    "### generate the measTech inclusion list\n",
    "all_unique_parents = generate_lineage_list(parent_inclusion_list, mapped_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2c0a582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68217\n"
     ]
    }
   ],
   "source": [
    "all_measTech_triples = mapped_triples.loc[mapped_triples['subject_id'].isin(all_unique_parents)]\n",
    "## Filter out any ontologies (like CHEBI) which may have been mapped due to ingestion via NCIT\n",
    "onto_keys = [\"NCIT\",\"topics\",\"EFO\",\"BAO\",\"OBI\",\"CHMO\",\"MMO\"]\n",
    "measTechOnly = pd.DataFrame(columns=[\"subject_id\",\"predicate_id\",\"object_id\",\"subject\",\"object\",\"predicate\"])\n",
    "for eachonto in onto_keys:\n",
    "    tmpdf = all_measTech_triples.loc[all_measTech_triples['subject_id'].astype(str).str.contains(eachonto) | \n",
    "                                     all_measTech_triples['object_id'].astype(str).str.contains(eachonto)]\n",
    "    measTechOnly = pd.concat((measTechOnly,tmpdf),ignore_index=True)\n",
    "\n",
    "measTechOnly.drop_duplicates(keep=\"first\",inplace=True)    \n",
    "measTechOnly.to_csv(os.path.join(result_path,'measTechOnly_mapped_triples.tsv'),sep='\\t',header=True)\n",
    "print(len(measTechOnly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b780f442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    subject_id  \\\n",
      "0  http://purl.obolibrary.org/obo/NCIT_C101294   \n",
      "1   http://purl.obolibrary.org/obo/NCIT_C16681   \n",
      "\n",
      "                                      predicate_id  \\\n",
      "0  http://www.w3.org/2000/01/rdf-schema#subClassOf   \n",
      "1  http://www.w3.org/2000/01/rdf-schema#subClassOf   \n",
      "\n",
      "                                         object_id                  subject  \\\n",
      "0  http://www.bioassayontology.org/bao#BAO_0002445  Whole Genome Sequencing   \n",
      "1  http://www.bioassayontology.org/bao#BAO_0000448                Histology   \n",
      "\n",
      "                         object   predicate  \n",
      "0             genotyping method  subClassOf  \n",
      "1  morphology assessment method  subClassOf  \n"
     ]
    }
   ],
   "source": [
    "print(measTechOnly.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f750524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960\n"
     ]
    }
   ],
   "source": [
    "print(len(all_measTech_triples.loc[all_measTech_triples['predicate']=='sameAs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb2e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f6685c7",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Investigating missing NCIT mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfdfa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(map_file_list)\n",
    "mapping_ontos = [\"NCIT\",\"topic\",\"EFO\",\"BAO\",\"OBI\",\"CHMO\",\"MMO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3df55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NCIT_map = pd.read_csv(os.path.join(map_path,map_file_list[6]),delimiter='\\t',header=0,index_col=0)\n",
    "\n",
    "relevant_df = pd.DataFrame(columns=['source_id','map_method','target_id','source_ontology'])\n",
    "for eachonto in mapping_ontos:\n",
    "    tmpdf = NCIT_map.loc[NCIT_map['target_id'].astype(str).str.contains(eachonto)]\n",
    "    tmp2df = tmpdf.loc[tmpdf['map_method']!=\"SAME_URI\"]\n",
    "    relevant_df = pd.concat((relevant_df,tmp2df),ignore_index=True)\n",
    "\n",
    "relevant_df.drop(columns=[\"source_ontology\"],inplace=True)\n",
    "\n",
    "relevant_df.to_csv(os.path.join(map_path,'NCIT_relevant_mappings.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb4702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_measTech_triples = pd.read_csv(os.path.join(result_path,'measTechOnly_mapped_triples.tsv'),delimiter='\\t',header=0,index_col=0)\n",
    "print(len(all_measTech_triples))\n",
    "print(all_measTech_triples.head(n=2))\n",
    "ncit_only_triples = all_measTech_triples.loc[all_measTech_triples['subject_id'].astype(str).str.contains(\"NCIT\")]\n",
    "print(len(ncit_only_triples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3060db2b",
   "metadata": {},
   "source": [
    "#### The number of NCIT triples is excessively low -- need to investigate why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "340746db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184165\n"
     ]
    }
   ],
   "source": [
    "ncit_onto = pd.read_csv(os.path.join(raw_path,'NCIT.csv'),header=0, usecols=['Class ID','Parents'])\n",
    "print(len(ncit_onto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ca7a649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Class ID  \\\n",
      "0  http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...   \n",
      "1  http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...   \n",
      "\n",
      "                                             Parents  \n",
      "0  http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...  \n",
      "1  http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...  \n",
      "http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#C173890\n"
     ]
    }
   ],
   "source": [
    "print(ncit_onto.head(n=2))\n",
    "print(ncit_onto.iloc[0]['Parents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75901f8e",
   "metadata": {},
   "source": [
    "#### The issue is that the urls are formatted differently -- need to convert purl version with ncicb version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8ea58f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations run:  0  ncit_parent_list:  4066  old_parent:  1068\n",
      "iterations run:  1  ncit_parent_list:  7459  old_parent:  4066\n",
      "iterations run:  2  ncit_parent_list:  11815  old_parent:  7459\n",
      "iterations run:  3  ncit_parent_list:  25438  old_parent:  11815\n",
      "iterations run:  4  ncit_parent_list:  27345  old_parent:  25438\n",
      "iterations run:  5  ncit_parent_list:  28363  old_parent:  27345\n",
      "iterations run:  6  ncit_parent_list:  28792  old_parent:  28363\n",
      "iterations run:  7  ncit_parent_list:  28917  old_parent:  28792\n",
      "iterations run:  8  ncit_parent_list:  28955  old_parent:  28917\n",
      "iterations run:  9  ncit_parent_list:  28957  old_parent:  28955\n",
      "iterations run:  10  ncit_parent_list:  28957  old_parent:  28957\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "ncit_list = [x for x in parent_inclusion_list if 'NCIT' in x]\n",
    "ncit_parent_list = [x.replace('http://purl.obolibrary.org/obo/NCIT_','http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#') for x in ncit_list]\n",
    "old_ncit_parent = len(set(ncit_parent_list))\n",
    "tmpdf = ncit_onto.loc[ncit_onto['Parents'].isin(ncit_parent_list)]\n",
    "tmplist = tmpdf['Class ID'].unique().tolist()\n",
    "new_ncit_parent = list(set(ncit_parent_list).union(set(tmplist)))\n",
    "while len(set(new_ncit_parent)) != old_ncit_parent:\n",
    "    tmpdf = ncit_onto.loc[ncit_onto['Parents'].isin(new_ncit_parent)]\n",
    "    tmplist = tmpdf['Class ID'].unique().tolist()  \n",
    "    old_ncit_parent = len(set(new_ncit_parent))\n",
    "    new_ncit_parent = list(set(new_ncit_parent).union(set(tmplist)))\n",
    "    print(\"iterations run: \",i,\" ncit_parent_list: \",len(new_ncit_parent),\" old_parent: \",old_ncit_parent)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a72318",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test a solution before implementing it\n",
    "\n",
    "df = ncit_onto.copy()\n",
    "df['Class ID'] = df['Class ID'].apply(lambda x: x.replace('http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#','http://purl.obolibrary.org/obo/NCIT_'))\n",
    "print(df.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48651acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88abc074",
   "metadata": {},
   "source": [
    "## Test bits of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e59f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "all_parent_list = [x for x in parent_inclusion_list]\n",
    "old_all_parent_list = len(set(all_parent_list))\n",
    "tmpdf = mapped_triples.loc[mapped_triples['object_id'].isin(parent_inclusion_list)]\n",
    "new_parent = tmpdf['subject_id'].unique().tolist()\n",
    "all_parent_list.extend(new_parent)\n",
    "while len(set(all_parent_list)) != old_all_parent_list:\n",
    "    tmpdf = mapped_triples.loc[mapped_triples['object_id'].isin(new_parent)]\n",
    "    new_parent = tmpdf['subject_id'].unique().tolist()\n",
    "    old_all_parent_list = len(set(all_parent_list))\n",
    "    all_parent_list.extend(new_parent)\n",
    "    i = i+1\n",
    "    print(\"iteration: \",i,\" old parents\",old_all_parent_list,\" all parents: \",len(set(all_parent_list)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

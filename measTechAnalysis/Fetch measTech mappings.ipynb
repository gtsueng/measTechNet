{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6ae8ed",
   "metadata": {},
   "source": [
    "## This notebook pulls mappings from NCBO Bioportal\n",
    "\n",
    "mappings endpoint documentation: https://data.bioontology.org/documentation#Mapping\n",
    "\n",
    "sample code: https://gist.github.com/callahantiff/a28fb3160782f42f104e9ec41553af0d\n",
    "\n",
    "NCBO sample code: https://github.com/ncbo/ncbo_rest_sample_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f0326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55c7a012",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = os.getcwd()\n",
    "data_path = os.path.join(script_path,'data')\n",
    "result_path = os.path.join(script_path,'result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b74eda1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.error, urllib.parse\n",
    "\n",
    "## Load the API key\n",
    "with open(os.path.join(script_path,'config.json'),'rb') as keyfile:\n",
    "    keyinfo = json.load(keyfile)\n",
    "    apikey = keyinfo['apikey']\n",
    "\n",
    "## Format the apikey for the header\n",
    "def get_json(url, apikey):\n",
    "    opener = urllib.request.build_opener()\n",
    "    opener.addheaders = [('Authorization', 'apikey token=' + apikey)]\n",
    "    return json.loads(opener.open(url).read())\n",
    "\n",
    "## Provide the list of ontologies to map\n",
    "onto_list = [\"BAO\",\"OBI\",\"EFO\",\"NCIT\",\"EDAM\",\"MMO\",\"CHMO\"]\n",
    "\n",
    "## Pull mapped pairs out of a paginated dictionary\n",
    "def get_mappings(onto_source,page_dict):\n",
    "    mappinglist = []\n",
    "    for eachcollection in page_dict['collection']:\n",
    "        tmpdict = {'source_ontology': onto_source,\n",
    "                   'source_id': eachcollection['classes'][0]['@id'],\n",
    "                   'map_method': eachcollection['source'],\n",
    "                   'target_id': eachcollection['classes'][1]['@id']}\n",
    "        mappinglist.append(tmpdict)\n",
    "    return mappinglist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25950388",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter the results to only mappings within ontologies of interest\n",
    "def filter_for_ontos(onto_list,mappingdf):\n",
    "    relevant_df = pd.DataFrame(columns=mappingdf.columns.tolist())\n",
    "    source_ont = mappingdf.iloc[0]['source_ontology']\n",
    "    target_list = [x for x in onto_list if x!=source_ont]\n",
    "    for eachtarget in target_list:\n",
    "        if eachtarget == \"EDAM\":\n",
    "            target_subset = mappingdf.loc[mappingdf['target_id'].str.contains(\"edamontology.org/topic\")]\n",
    "        else:\n",
    "            target_subset = mappingdf.loc[mappingdf['target_id'].str.contains(eachtarget)]\n",
    "            if eachtarget == \"MMO\":\n",
    "                tempsubset = target_subset.copy()\n",
    "                target_subset = tempsubset.loc[~tempsubset['target_id'].str.contains(\"EMMO\")]\n",
    "        relevant_df = pd.concat(([relevant_df,target_subset]),ignore_index=True)\n",
    "    return relevant_df\n",
    "\n",
    "def download_mappings(apikey,onto_list,onto_subset,starting_page):\n",
    "    for each_onto in onto_subset:\n",
    "        print(\"now downloading mappings from: \",each_onto, \"page: \",starting_page)\n",
    "        allmappinglist = []\n",
    "        ontologymap = f\"https://data.bioontology.org/ontologies/{each_onto}/mappings\"\n",
    "        r = get_json(ontologymap,apikey)\n",
    "        tmpmapping = get_mappings(each_onto, r)\n",
    "        allmappinglist.extend(tmpmapping)\n",
    "        if starting_page == 0:\n",
    "            print(r[\"links\"][\"nextPage\"])\n",
    "            page = get_json(r['links'][\"nextPage\"],apikey)\n",
    "            print(page[\"links\"][\"nextPage\"])\n",
    "            tmpmapping = get_mappings(each_onto, page)\n",
    "            allmappinglist.extend(tmpmapping)\n",
    "        else:\n",
    "            page = starting_page\n",
    "            print(page[\"links\"][\"nextPage\"])\n",
    "            tmpmapping = get_mappings(each_onto, page)\n",
    "            allmappinglist.extend(tmpmapping)\n",
    "        next_page = page\n",
    "        while next_page:\n",
    "            next_page = page[\"links\"][\"nextPage\"]\n",
    "            tmpmapping = get_mappings(each_onto, page)\n",
    "            allmappinglist.extend(tmpmapping)\n",
    "            allmappingcopy = [x for x in allmappinglist]\n",
    "            if next_page:\n",
    "                page = get_json(next_page,apikey)\n",
    "                print(page[\"links\"][\"nextPage\"])\n",
    "        mappingdf = pd.DataFrame(allmappingcopy)\n",
    "        print(len(mappingdf))\n",
    "        relevant_df = filter_for_ontos(onto_list,mappingdf)\n",
    "        relevant_df.to_csv(os.path.join(result_path,\"mappings\",f\"{each_onto}_mappings.tsv\"), sep='\\t', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd79aa14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDAM\n",
      "30\n",
      "36\n",
      "11\n",
      "3\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "## get mapping statistics\n",
    "ontology = onto_list[4]\n",
    "print(ontology)\n",
    "r = get_json(f\"https://data.bioontology.org/mappings/statistics/ontologies/{ontology}\",apikey)\n",
    "print(r['EFO'])\n",
    "print(r['OBI'])\n",
    "print(r['CHMO'])\n",
    "print(r['MMO'])\n",
    "print(r['BAO'])\n",
    "#print(r['EDAM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ad6c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confirm the reverse mappings\n",
    "ontology = onto_list[6]\n",
    "print(ontology)\n",
    "r = get_json(f\"https://data.bioontology.org/mappings/statistics/ontologies/{ontology}\",apikey)\n",
    "print(r['EDAM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0597587d",
   "metadata": {},
   "source": [
    "## Trouble-shooting\n",
    "The mappings between EDAM \"electron microscopy\" and CHMO \"electron microscopy\" exists, but isn't being pulled by the current script. Investigate what's wrong\n",
    "* The filter was not including EDAM properly\n",
    "* The fixes to the downloader appeared to have worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a37f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_subset = [\"MMO\"]\n",
    "starting_page = 0\n",
    "for each_onto in onto_subset:\n",
    "    print(\"now downloading mappings from: \",each_onto, \"page: \",starting_page)\n",
    "    allmappinglist = []\n",
    "    ontologymap = f\"https://data.bioontology.org/ontologies/{each_onto}/mappings\"\n",
    "    r = get_json(ontologymap,apikey)\n",
    "    print(r['links']['nextPage'])\n",
    "    if starting_page == 0:\n",
    "        page = get_json(r['links'][\"nextPage\"],apikey)\n",
    "        #print(page['links']['nextPage'])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939e5b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Download all mappings from all ontologies of interest\n",
    "onto_list = [\"MMO\",\"CHMO\",\"BAO\",\"OBI\",\"EDAM\",\"EFO\",\"NCIT\"]\n",
    "onto_subset = [\"MMO\",\"CHMO\",\"BAO\",\"OBI\",\"EDAM\"]\n",
    "download_mappings(apikey,onto_list,onto_subset,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156d6a53",
   "metadata": {},
   "source": [
    "### Download mappings from specific ontologies of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702a10a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Download extensive mappings\n",
    "each_onto = \"EFO\"#onto_list[4]\n",
    "i = 0\n",
    "allmappinglist = []\n",
    "relevant_df = pd.DataFrame(columns=[\"source_ontology\",\"source_id\",\"map_method\",\"target_id\"])\n",
    "ontologymap = f\"https://data.bioontology.org/ontologies/{each_onto}/mappings\"\n",
    "r = get_json(ontologymap,apikey)\n",
    "starting_page = 0\n",
    "tmpmapping = get_mappings(each_onto, r)\n",
    "allmappinglist.extend(tmpmapping)\n",
    "if starting_page == 0:\n",
    "    print(r[\"links\"][\"nextPage\"])\n",
    "    page = get_json(r['links'][\"nextPage\"],apikey)\n",
    "    print(page[\"links\"][\"nextPage\"])\n",
    "    tmpmapping = get_mappings(each_onto, page)\n",
    "    allmappinglist.extend(tmpmapping)\n",
    "else:\n",
    "    page = starting_page\n",
    "    print(page[\"links\"][\"nextPage\"])\n",
    "    tmpmapping = get_mappings(each_onto, page)\n",
    "    allmappinglist.extend(tmpmapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2db085",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save current progress to resume later\n",
    "from datetime import datetime\n",
    "import json\n",
    "current_progress = {\"page\":page,\n",
    "                     \"next_page\": next_page,\n",
    "                     \"i\":i,\n",
    "                     \"each_onto\":each_onto,\n",
    "                     \"allmappinglist\": allmappinglist}\n",
    "\n",
    "progress_date = datetime.strftime(datetime.now(),\"%Y-%m-%d\")\n",
    "\n",
    "with open(os.path.join(result_path,\"mappings\",f\"{each_onto}_progress({progress_date}).json\"), \"w\") as outwrite:\n",
    "    outwrite.write(json.dumps(current_progress))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8778ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resume progress\n",
    "from datetime import datetime\n",
    "import json\n",
    "each_onto = \"EFO\" #onto_list[4]\n",
    "progress_date = \"2024-10-19\"\n",
    "with open(os.path.join(result_path,\"mappings\",f\"{each_onto}_progress({progress_date}).json\"), \"r\") as infile:\n",
    "    parameter_dict = json.load(infile)\n",
    "\n",
    "print(parameter_dict)\n",
    "page = parameter_dict[\"page\"]\n",
    "next_page = parameter_dict[\"next_page\"]\n",
    "i = parameter_dict[\"i\"]\n",
    "each_onto = parameter_dict[\"each_onto\"]\n",
    "allmappinglist = parameter_dict[\"allmappinglist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e95921",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbedee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "while next_page:\n",
    "    next_page = page[\"links\"][\"nextPage\"]\n",
    "    tmpmapping = get_mappings(each_onto, page)\n",
    "    allmappinglist.extend(tmpmapping)\n",
    "    allmappingcopy = [x for x in allmappinglist]\n",
    "    if next_page:\n",
    "        page = get_json(next_page,apikey)\n",
    "        #print(page[\"links\"][\"nextPage\"])\n",
    "    if len(allmappinglist) >= 5000: ## Export when the number of mappings exceeds 1000\n",
    "        i=i+1\n",
    "        mappingdf = pd.DataFrame(allmappinglist)\n",
    "        mappingdf.to_csv(os.path.join(result_path,\"mappings\",f\"{each_onto}_all_mappings_{str(i)}.tsv\"), sep='\\t', header=True)\n",
    "        tmp_df = filter_for_ontos(onto_list,mappingdf)\n",
    "        if len(tmp_df) > 0:\n",
    "            relevant_df = pd.concat((relevant_df,tmp_df), ignore_index = True)\n",
    "        allmappinglist = [] ## Reset the mappings list\n",
    "        print(i, \" file dumped. Current page: \",next_page)\n",
    "\n",
    "mappingdf = pd.DataFrame(allmappingcopy)\n",
    "print(len(mappingdf))\n",
    "relevant_df = filter_for_ontos(onto_list,mappingdf)        \n",
    "relevant_df.to_csv(os.path.join(result_path,\"mappings\",f\"{each_onto}_relevant_mappings.tsv\"), sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e965618",
   "metadata": {},
   "source": [
    "### Download mappings for specific nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9100a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get list of measTechOnly terms\n",
    "script_path = os.getcwd()\n",
    "parent_path = os.path.abspath(os.path.join(script_path, os.pardir))\n",
    "measTechOnlyFile = os.path.join(parent_path,\"measTechNet\",\"results\",\"measTechOnly_mapped_triples.tsv\")\n",
    "measTechOnlyDF = pd.read_csv(measTechOnlyFile)\n",
    "measTechNodes = list(set(measTechOnlyDF['subject_id'].unique().tolist()).union(set(measTechOnlyDF['object_id'].unique().tolist())))\n",
    "\n",
    "mappings_df = pd.DataFrame(columns=[\"source_ontology\",\"source_id\",\"map_method\",\"target_id\"])\n",
    "for eachonto in onto_list:\n",
    "    allmappinglist = []\n",
    "    if eachonto == \"EDAM\":\n",
    "        classlist = [x for x in measTechNodes if \"edamontology.org/topic\" in x]\n",
    "    else:\n",
    "        classlist = [x for x in measTechNodes if eachonto in x]\n",
    "        \n",
    "    for eachclass in classlist:\n",
    "        classmap = f\"https://data.bioontology.org/ontologies/{eachonto}/classes/{eachclass}/mappings\"\n",
    "        r = get_json(ontologymap,apikey)\n",
    "        page = starting_page\n",
    "        tmpmapping = get_mappings(each_onto, page)\n",
    "        allmappinglist.extend(tmpmapping)\n",
    "        \n",
    "    mappingdf = pd.DataFrame(allmappinglist)\n",
    "    relevant_df = filter_for_ontos(onto_list,mappingdf)\n",
    "    relevant_df.to_csv(os.path.join(result_path,\"class-based_mappings\",f\"{each_onto}_mappings.tsv\"), sep='\\t', header=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12005b1",
   "metadata": {},
   "source": [
    "## Filtering general ontology mapping files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2cfa0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d02b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = os.getcwd()\n",
    "data_path = os.path.join(script_path,'data')\n",
    "result_path = os.path.join(script_path,'result')\n",
    "map_path = os.path.join(result_path,'mappings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c58c3cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Provide the list of ontologies to map\n",
    "onto_list = [\"BAO\",\"OBI\",\"EFO\",\"NCIT\",\"EDAM\",\"MMO\",\"CHMO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b486044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EFO_progress(2024-10-20).json\n",
      "8177\n"
     ]
    }
   ],
   "source": [
    "## load mapping files\n",
    "onto_2_map = onto_list[2]\n",
    "onto_file_list = os.listdir(map_path)\n",
    "onto_only_list = [x for x in onto_file_list if onto_2_map in x]\n",
    "\n",
    "allmappinglist = pd.read_csv(os.path.join(map_path,onto_only_list[0]),delimiter='\\t',header=0,index_col=0)\n",
    "for eachfile in onto_only_list:\n",
    "    mappingdf = pd.read_csv(os.path.join(map_path,eachfile),delimiter='\\t',header=0,index_col=0)\n",
    "    try:\n",
    "        tmpdf = filter_for_ontos(onto_list,mappingdf)\n",
    "        if len(tmpdf)>0:\n",
    "            allmappinglist = pd.concat((allmappinglist,tmpdf),ignore_index=True)\n",
    "    except:\n",
    "        print(eachfile)\n",
    "clean_mappings = filter_for_ontos(onto_list,allmappinglist)\n",
    "clean_mappings.drop_duplicates(keep=\"first\",inplace=True)\n",
    "print(len(clean_mappings))\n",
    "clean_mappings.to_csv(os.path.join(map_path,f'{onto_2_map}_mappings.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cacf17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpmapping = get_mappings(each_onto, page)\n",
    "allmappinglist.extend(tmpmapping)\n",
    "allmappingcopy = [x for x in allmappinglist]\n",
    "    mappingdf = pd.DataFrame(allmappinglist)\n",
    "    mappingdf.to_csv(os.path.join(result_path,\"mappings\",f\"{each_onto}_all_mappings_{str(i)}.tsv\"), sep='\\t', header=True)\n",
    "    #tmp_df = filter_for_ontos(onto_list,mappingdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbcd73c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f69f507",
   "metadata": {},
   "source": [
    "## Figuring out the data structure of a result from the API\n",
    "\n",
    "The organization of the mappings appear to be as follows:\n",
    "A result from the API ontology/mappings endpoint:\n",
    "* r.keys:  dict_keys(['page', 'pageCount', 'totalCount', 'prevPage', 'nextPage', 'links', 'collection'])\n",
    "  * r['collection'].keys():  dict_keys(['id', 'source', 'classes', 'process', '@id', '@type'])\n",
    "    * r['collection'][0]['classes'].keys:  dict_keys(['@id', '@type', 'links', '@context'])\n",
    "\n",
    "Where each pair of mapped terms appear to be listed under 'classes' with classes[0] being the term in the source ontology and classes[1] being the term from a different ontology\n",
    "\n",
    "The source is how two terms were mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22aa056",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test to see that an API request is working\n",
    "REST_URL = \"http://data.bioontology.org\"\n",
    "term = \"survey\"\n",
    "r = get_json(REST_URL + \"/search?q=\" + term,apikey)[\"collection\"]\n",
    "#print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43302bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing the class mapping endpoint\n",
    "ontology_shorthand = 'BRO'\n",
    "classurl = 'http%3A%2F%2Fbioontology.org%2Fontologies%2FBiomedicalResourceOntology.owl%23Ontology_Development_and_Management'\n",
    "classmap = f\"https://data.bioontology.org/ontologies/{ontology_shorthand}/classes/{classurl}/mappings\"\n",
    "r = get_json(classmap,apikey)\n",
    "print('r[0].keys: ', r[0].keys())\n",
    "print('r[0][classes][0].keys: ', r[0]['classes'][0].keys())\n",
    "print('r[0][classes][0][links]: ', r[0]['classes'][0]['links'].keys())\n",
    "print(r[0]['classes'][0]['links']['descendants'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b0e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the ontology mapping end point\n",
    "ontology_shorthand = 'MMO'\n",
    "classurl = 'http://purl.obolibrary.org/obo/MMO_0000000'\n",
    "ontologymap = f\"https://data.bioontology.org/ontologies/{ontology_shorthand}/mappings\"\n",
    "r = get_json(ontologymap,apikey)\n",
    "\n",
    "print(\"r.keys: \",r.keys())\n",
    "print(\"r['links']: \", r['links'])\n",
    "print(\"r['page']: \", r['page'])\n",
    "print(\"r['collection'].keys(): \", r['collection'][0].keys())\n",
    "print(\"r['collection'][0]['classes'].keys: \", r['collection'][0]['classes'][0].keys())\n",
    "print(\"collection id: \",r['collection'][0][\"@id\"])\n",
    "print(\"class id: \", r['collection'][0]['classes'][1][\"@id\"])\n",
    "print(\"class type: \", r['collection'][0]['classes'][1][\"@type\"])\n",
    "print(\"class context: \", r['collection'][0]['classes'][1][\"@context\"])\n",
    "print(\"number of classes: \",len(r['collection'][0]['classes']))\n",
    "\n",
    "print(r['pageCount'])\n",
    "for eachcollection in r['collection'][0:3]:\n",
    "    #print(len(eachcollection['classes']))\n",
    "    print(eachcollection['classes'][0]['@id'],eachcollection['source'],eachcollection['classes'][1]['@id'])\n",
    "    print(eachcollection['classes'][0]['@context'],eachcollection['classes'][1]['@context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a45018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the use of pagination\n",
    "page = get_json(r['links'][\"nextPage\"],apikey)\n",
    "allmappinglist = []\n",
    "# Iterate over the available pages adding labels from all classes\n",
    "# When we hit the last page, the while loop will exit\n",
    "next_page = page\n",
    "while next_page:\n",
    "    next_page = page[\"links\"][\"nextPage\"]\n",
    "    tmpmapping = get_mappings(\"MMO\", page)\n",
    "    allmappinglist.extend(tmpmapping)\n",
    "    if next_page:\n",
    "        page = get_json(next_page,apikey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b82e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappingdf = pd.DataFrame(mappinglist)\n",
    "print(mappingdf.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a60446",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mappingdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f33862",
   "metadata": {},
   "source": [
    "## Test the use of pageCount and baseurls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c9e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download extensive mappings\n",
    "each_onto = \"EFO\"#onto_list[4]\n",
    "i = 0\n",
    "allmappinglist = []\n",
    "relevant_df = pd.DataFrame(columns=[\"source_ontology\",\"source_id\",\"map_method\",\"target_id\"])\n",
    "ontologymap = f\"https://data.bioontology.org/ontologies/{each_onto}/mappings\"\n",
    "r = get_json(ontologymap,apikey)\n",
    "pageNumber = 0\n",
    "pageCount = r[\"pageCount\"]\n",
    "tmpmapping = get_mappings(each_onto, r)\n",
    "allmappinglist.extend(tmpmapping)\n",
    "print(pageCount)\n",
    "if pageNumber == 0:\n",
    "    page = get_json(r['links'][\"nextPage\"],apikey)\n",
    "    tmpmapping = get_mappings(each_onto, page)\n",
    "    allmappinglist.extend(tmpmapping)\n",
    "    pageNumber = pageNumber + 1\n",
    "    next_page = f\"https://data.bioontology.org/ontologies/{each_onto}/mappings?page={pageNumber}\"\n",
    "else:\n",
    "    baseurl = f\"https://data.bioontology.org/ontologies/{each_onto}/mappings?page={pageNumber}\"\n",
    "    page = get_json(baseurl,apikey)\n",
    "    tmpmapping = get_mappings(each_onto, page)\n",
    "    allmappinglist.extend(tmpmapping)\n",
    "    pageNumber = pageNumber + 1\n",
    "    next_page = baseurl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04f84255",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save current progress to resume later\n",
    "from datetime import datetime\n",
    "import json\n",
    "current_progress = {\"pageNumber\": pageNumber,\n",
    "                    \"pageCount\": pageCount,\n",
    "                    \"next_page\": next_page,\n",
    "                    \"i\":i,\n",
    "                    \"each_onto\":each_onto,\n",
    "                    \"allmappinglist\": allmappinglist}\n",
    "\n",
    "progress_date = datetime.strftime(datetime.now(),\"%Y-%m-%d\")\n",
    "\n",
    "with open(os.path.join(result_path,\"mappings\",f\"{each_onto}_progress({progress_date}).json\"), \"w\") as outwrite:\n",
    "    outwrite.write(json.dumps(current_progress))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9044d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resume progress (if saved)\n",
    "from datetime import datetime\n",
    "import json\n",
    "each_onto = \"EFO\" #onto_list[4]\n",
    "progress_date = \"2024-10-19\"\n",
    "with open(os.path.join(result_path,\"mappings\",f\"{each_onto}_progress({progress_date}).json\"), \"r\") as infile:\n",
    "    parameter_dict = json.load(infile)\n",
    "\n",
    "next_page = parameter_dict[\"next_page\"]\n",
    "pageNumber = parameter_dict[\"pageNumber\"]\n",
    "pageCount = parameter_dict[\"pageCount\"]\n",
    "i = parameter_dict[\"i\"]\n",
    "each_onto = parameter_dict[\"each_onto\"]\n",
    "allmappinglist = parameter_dict[\"allmappinglist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a77de8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10128 10292\n"
     ]
    }
   ],
   "source": [
    "print(pageNumber, pageCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03292395",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resume progress (if not saved)\n",
    "from datetime import datetime\n",
    "import json\n",
    "each_onto = \"EFO\" #onto_list[4]\n",
    "pageNumber = 4403\n",
    "i = 47\n",
    "next_page = next_page = f\"https://data.bioontology.org/ontologies/{each_onto}/mappings?page={pageNumber}\"\n",
    "allmappinglist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba3210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageNumber=4403\n",
    "next_page = \"https://data.bioontology.org/ontologies/EFO/mappings?page=4403\"\n",
    "i=47\n",
    "print(pageNumber, pageCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1fffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_df = tmp_df.copy()\n",
    "print(len(relevant_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bc5186d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while pageNumber < pageCount:\n",
    "    page = get_json(next_page,apikey)\n",
    "    tmpmapping = get_mappings(each_onto, page)\n",
    "    allmappinglist.extend(tmpmapping)\n",
    "    allmappingcopy = [x for x in allmappinglist]\n",
    "    pageNumber = pageNumber+1\n",
    "    next_page = f\"https://data.bioontology.org/ontologies/{each_onto}/mappings?page={pageNumber}\"\n",
    "    if len(allmappinglist) >= 5000: ## Export when the number of mappings exceeds 1000\n",
    "        i=i+1\n",
    "        mappingdf = pd.DataFrame(allmappinglist)\n",
    "        mappingdf.to_csv(os.path.join(result_path,\"mappings\",f\"{each_onto}_all_mappings_{str(i)}.tsv\"), sep='\\t', header=True)\n",
    "        #tmp_df = filter_for_ontos(onto_list,mappingdf)\n",
    "        #if len(tmp_df) > 0:\n",
    "        #    relevant_df = pd.concat((relevant_df,tmp_df), ignore_index = True)\n",
    "        allmappinglist = [] ## Reset the mappings list\n",
    "        print(i, \" file dumped. Current page: \",next_page)\n",
    "\n",
    "mappingdf = pd.DataFrame(allmappinglist)\n",
    "mappingdf.to_csv(os.path.join(result_path,\"mappings\",f\"{each_onto}_all_mappings_{str(i)}.tsv\"), sep='\\t', header=True)\n",
    "#mappingdf = pd.DataFrame(allmappingcopy)\n",
    "#print(len(mappingdf))\n",
    "#relevant_df = filter_for_ontos(onto_list,mappingdf)        \n",
    "#relevant_df.to_csv(os.path.join(result_path,\"mappings\",f\"{each_onto}_relevant_mappings.tsv\"), sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0959b14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
